---
title: ""
biblatex: 1
biblio-style: authoryear,backend=biber
biblatexoptions: 
  - maxbibnames = 20
  - maxcitenames = 2
  - doi = false
  - isbn = false
  - uniquelist = false
bibliography: "bibliography.bib"
geometry: margin=1in 
fontsize: 12pt
documentclass: article
classoption: letterpaper
link-citations: true
font: "Times New Roman"
---



<!-- 7 pages in the full proposal -->

## Overview

There is growing interest in developing a simple, intuitive air quality index that simultaneously accounts for the health effects of multiple air pollutants [@dominici2010protecting; @stieb2008new; @bopp2018something]. Health effects of air pollution depend on the composition of pollutants in the air, not simply the levels of a single pollutant [@dominici2010protecting]. An air quality index that reflects this understanding should account for the levels and relative contributions of each air pollutant in the ambient air. In this proposal, we will improve statistical methods for conducting inference on the health effects of simultaneous exposure to multiple environmental pollutants, with a focus on quantifying short-term effects of poor air quality on health outcomes at the population level.

The *constrained groupwise additive index model* (cGAIM), introduced by @xia2006cumulative, is a vehicle for providing a multi-pollutant health index. Writing $\lambda_{it}$ for the risk of a particular outcome for an individual $i$ on day $t$,  the cGAIM is
$$
  \lambda_{it} = \exp\left[ X_{it}^{T}\beta + s(\alpha^{T} Z_{it} ) + f_{1}( W_{1it} ) + \ldots + f_{K}( W_{Kit} ) \right].
$$
The $\beta$ parameters are the fixed effects of the potentially time-varying linear covariates $X_{it}$, and $f_{1}, \ldots, f_{K}$ are smooth functions that account for potential confounding variables $W_{kit}$ ($k = 1 \ldots K$). The distinguishing feature of cGAIM is the smooth function $s$ whose argument is a linear combination of covariates $Z_t$, which might be PM 2.5 and Ozone. The $\alpha$ parameter is a vector of weights on the entries of $Z_t$, and gives their relative contributions.   The smooth functions $s$ and the $f_k$ might be composed of spline functions or Gaussian processes such as random walks.  

While cGaim has until now been used with daily case counts as the response variable, we will employ case crossover models, which have seen increased attention in the air pollution literature [@wei2019short; @stringer2020approximate].  These models define one or more *control days* for each case, for example the same day of the week on the previous two weeks, and use a partial likelihood for the probability the event occurs on the case day rather than the control days.  The advantage of case crossover models is any risk factors which vary slowly or not at all, or are the same on the case and control days, are automatically adjusted for.  The challenge introduced by case crossover models is the likelihood depends on non-linear combinations of the latent variables. 


Estimating $\alpha$ is the main statistical challenge with the cGAIM, which @pierre2020aim 
accomplish with frequentist inference methods that use sequential quadratic programming.  We will develop a Bayesian methodology for inference with the cGAIM --- the bcGAIM --- which will fully quantify the uncertainty around $\alpha$ and propagate the uncertainty into inference on $s$.  While there is consistent evidence that air pollution increases daily incidence of adverse health outcomes, we expect that information contained in data on the effects of specific combinations of pollutants at different lags is weak.  The bcGAIM will identify both what we can and cannot infer from historical health and pollution data, a feature which will become increasingly important as the dimensionality of $\alpha$ and $Z_{it}$ increase.


## Outcomes and applications

This project brings together the methodological components of several interdisciplinary and collaborative research activities which the four investigators have been engaging in independently of each other.  

The primary driver of this research is the need for an improved air quality warning system, which Health Canada and the Institut national de santé publique du Québec have separately approached Drs Brown and Chebana (respectively) about.  Currently the Canadian AQHI is composed of relative risks estimated from cohort studies, and estimated risks for individual pollutants are summed to create a log-relative risk which is in turn converted to a 10-point scale.  This is likely to over-estimate risk,  @franklin2008impact found that the effect of ozone on non-accidental mortality was "substantially reduced" after adjusting for particle sulfate and @liu2019ambient found significant differences in the percentage change of all-cause mortality attributable to PM~2.5~ and PM~10~ after adjusting for NO~2~ or SO~2~. Furthermore, there is evidence that some health outcomes are nonlinearly related to pollution measurements [@feng2016impact].  Dr. Brown's group has developed a linear multi-pollutant case/crossover model [@guowen2020multi] whereas Dr. Chebana has used a cGAIM with a frequentist time series model [@pierre2020aim].  The proposed bcGAIM is a natural extension of, and merging of, these two methods.  

A second driver of this project is the environmental epidemiology research undertaken by the investigators in collaboration with health science researchers.  The Centre for Global Health Research, where Dr. Brown is partly based, has history of producing papers on global mortality in high-impact journals.  The Million Deaths Study in India has 13 years worth of cause-specific mortality data geocoded to point locations and with smoking and diet information about the deceased and from healthy respondents.  Dr. Franklin has a number of highly cited papers on air quality and mortality in environmental health journals.  With our collaborators, including Prabhat Jha in Toronto and Daniel Rainham at Dalhousie, we will use bcGAIM to produce papers for the top-ranked medical journals.

The third motivation for this CRT is the surge in availability of daily mortality data brought on by the COID-19 pandemic.  
The relationship between daily COVID-19 deaths and air pollution levels has recently become an active area of research.  @wu2020exposure find that a 1 $\mu$g increase in long-term exposure to ambient PM~2.5~ increases the COVID-19 mortality rate by 15%.  We will relate COVID-19 incidence and mortality to air pollution in major urban centres worldwide, where possible focusing on deaths outside long-term care homes.  


A key reason the bcGAIM model is ideal for the above problems is it will produce parameters which are interpretable.  The $\alpha$ coefficients give the relative importance of each pollutant (at each lag), and $s(\cdot)$ is the relative risk from a basket of exposures.  Unsupervised methods such as principle components analysis and clustering can be difficult to interpret [@davalos2017current]. A popular nonparametric method is Bayesian Kernel Machine Regression (BKMR), which models an exposure-response surface via a kernel function [@bobb2015bayesian]. Using a hierarchical Bayesian variable selection method, it can select one pollutant from a group of correlated ones, and is interpreted by visualizing cross-sections of a potentially high-dimensional exposure-response surface. The bcGAIM will provide similar flexibility to the BKMR, while being able to meet the communication needs of inter-disciplinary research teams.

## Methods

The bcGAIM will make four methodological advancements for modeling  health effects of mixtures of exposures. These are:

1. develop bcGAIM, a Bayesian inference methodology for high dimensional cGAIM's in case-crossover models;
1. create an efficient, non-iterative computational algorithm for bcGAIM's based on Laplace approximations;
1. develop non-parametric forms of the dose-response effect which encourage or enforce monotonicity; and
1. engage in interdisciplinary and applied research projects with our subject-area collaborators.

### bcGAIM


identifiability, mixing, priors, case-crossover

Regarding the first methodological innovation, the cGAIM uses an iterative two-step optimization scheme. In the first step $\alpha$ is updated using a quadratic program, while in the second $s$ is updated using the methodology from @pya2015shape. Any linear constraint can be placed on $\alpha$. For example, it can be constrained to have non-negative components that sum to one, so that it is a vector of weights. Once estimated, these weights give the relative contribution of each component to the outcome of interest. The cGAIM can also constrain the shape of $s$ by requiring it to, for example, be monotonic or convex. 

For the second extension, the bcGAIM It will initially be implemented in Stan, a statistical modeling language that facilitates iterative model development [@carpenter2017stan]. For the multi-pollutant model, doing so will allow us to extend the bcGAIM to additional pollutants, additional lags for pollutants, and additional smooth functions $s$. We expect that $\alpha$ will not always be well identified, and the results will be sensitive to model assumptions and prior distributions. A major task in this component of the research will be to find reparametrizations and multivariable prior distributions that enable prior elicitation from subject-area specialists. After bcGAIM is implemented for a three-dimensional $\alpha$ (with covariates O~3~, PM~2.5~, and NO~2~ at two day lags), additional time lags will be added with the resulting $\alpha$ being 9-12 dimensional. The computational and methodological challenges at this stage are expected to be significant, and parallelizing the algorithm on cloud platforms will be used to dramatically increase the number of candidate values of $\alpha$ considered.



### algorithm

For the fourth methodological extension, we will develop non-MCMC inference methods similar in spirit to INLA [@rue2009approximate]. The Latent Gaussian approximation in INLA separates the parameter space into covariance parameters $\theta$ and linear predictors $\eta = (\beta, \theta, f)$, and considers $\pi( \eta | Y, \theta)$, $\pi( \theta | Y)$, and $\pi(\eta | Y) = \int \pi(\eta | Y, \theta) \pi(\theta | Y) d\theta$ (the last one numerically). INLA performs approximate inference on $\theta$ by estimating $\phi( \theta | Y, \phi )$ with a normal distribution with mean $\theta^{*}$ and variance $\Sigma^{*}$. If the likelihood is log-concave and Gaussian priors are used, $\pi( \theta | Y, \phi )$ is unimodal and is well-approximated by the Laplace approximation. In @margossian2020hamiltonian, the authors estimate $\pi( \theta | Y, \phi)$ with the Laplace approximation and $\pi( \theta | T)$ with Hamiltonian Monte Carlo. They find that this performs well for their examples, both of which have log-concave likelihoods.

Let us translate this reasoning to the bcGAIM, which has link function $g(\lambda_{t}) = X^{t}\beta + s( \alpha^{T} Z_{t}) + f_{1}(W_{1,t}) + \ldots + f_{K}(W_{K,t})$. Note that conditional on $\alpha$, $\alpha^{T}Z_{t}$ is known. Thus, we can simplify the estimation problem by considering parameters $\phi$, $\theta$, and $\alpha$ and estimating $\pi( \eta | Y, \theta, \alpha)$, $\pi( \alpha | Y, \theta )$, $\pi(\theta | Y)$, and $\pi(\eta | Y) = \int \pi(\eta | Y, \theta, \alpha) \pi( \alpha | Y, \theta ) \pi( \theta | Y) d\theta d\alpha$ (the last one numerically). The first and third densities in the integrand, $\pi(\eta | Y, \theta, \alpha)$ and $\pi( \theta | Y)$ are well-suited to the Laplace approximation while $\pi(\alpha | Y,\theta )$ can be estimated using HMC. Introducing two Laplace approximations will lessen the computational burden, and enable us to fit a hierarchical bcGAIM model to air pollution data. This will allow us to produce national estimates of air quality while fitting the bcGAIM to over 25 regions across Canada, each with over 6,000 daily observations, an otherwise daunting computational task. Therefore, this non-MCMC inference method will provide significant computational and ease-of-use benefits, and will expand the types of problems and number of users who can use the bcGAIM methodology. To facilitate use by other researchers, all bcGAIM software will be released in an R package.




### monotonicity

The cGAIM considers both constraints and groupwise additive index terms, while much of the existing literature only considers groupwise additive terms. For example, @hardle1993optimal focus on a single index and minimizes a least-squares criteria where  a trimmed version of a leave-one-out Nadaraya-Watson estimator of $s$ is used to jointly choose the bandwidth parameter and estimate $\alpha$. For several indices, @wang2015estimation minimize a least-squares criteria via a two-stage estimation procedure. They derive large-sample properties of this least-squares estimator, and propose a penalized least-squares estimator for sparse high-dimensional settings. A few other papers propose alternative objective function similar to least-squares but none of these papers consider constrained estimation [@li2010groupwise; @guo2015groupwise; @wang2017robust]. 



For the third innovation, a major task is to develop Gaussian process priors, such as random walks, for shape-constrained Bayesian inference on $s$. In addition to having desirable statistical properties, the prior should be simple and interpretable so that it can be elicited from subject-area experts. One approach to achieving this is a nested model approach. Consider a prior $\pi(\phi)$ on $s$ that encourages monotonicity. Viewing the bcGAIM with $s$ monotonic as nested within the bcGAIM with $s$ unconstrained, we want $\phi$ to control how strongly $s$ is encouraged towards monotonicity. Moreover, how strongly $\pi(\phi)$ encourages monotonicity should be easy to communicate visually. This will facilitate prior elicitation and improve our ability to communicate modeling results. 

Priors can have subtle negative effects on the posterior, which cam be difficult to discern in hierarchical models and/or high dimensional settings. For example, a truncated multivariate normal (tMVN) prior can induce monotonicity if placed on the coefficients of a basis expansion of $s$ [@maatouk2017gaussian]. However, a tMVN prior subject to linear constraints places negligible mass in near-flat regions of $s$ in high-dimensional settings. This is remedied in @zhou2020truncated, who introduce a scale parameter on the coordinates of the tMVN, and use the half-Cauchy distribution as a shrinkage prior on these parameters. We will perform iterative development of our priors, conducting simulation studies to verify that they do not introduce undesirable side effects. 

<!-- There is a vast literature on shape-constrained inference. We briefly highlight some frequentist papers below. In an influential paper, @ramsay1988monotone proposes an unconstrained inference method for monotone functions, and @meyer2008inference extends these results to additional shape constraints. @mammen1999smoothing describes a correspondence between fitting a constrained function and fitting then projecting an unconstrained function onto a constrained space; we discuss a Bayesian projection method by @lin2014bayesian below. Considering constrained inference, @sysoev2019smoothed use a penalized likelihood to enforce monotonicity while maintaining smoothness properties, while @leitenstorfer2007generalized constrains the coefficients of the basis expansion to enforce monotonicity. Finally, there is an emerging branch of machine learning literature focusing on shape constrained inference, which shares our project's goal of improving model interpretability in big data settings but is somewhat orthogonal to Bayesian inference methods we will be using [@sill1998monotonic; @gupta2016monotonic; @wehenkel2019unconstrained]. -->

There is a vast literature on Bayesian shape-constrained inference for Gaussian processes. The distribution of a constrained Gaussian process is no longer a Gaussian process. However, the derivative of a Gaussian process is. @riihimaki2010gaussian use this to enforce monotonicity under a data augmentation scheme where derivatives are required to be positive at the virtual locations. @agrell2019gaussian and @wang2016estimating find that a relatively small number of virtual observations are needed to to ensure the shape constraint holds globally with high probability. However, we have found that the effect of air pollution can substantially deviate from monotonicity [@kamal2020trends]. Also, our air pollution data sets have over 6,000 daily observations per region, and adding more virtual observations may not be computationally feasible. Therefore, data augmentation is not optimal for this project.

Another approach is to approximate the Gaussian process with a basis expansion and constrain the coefficients of that expansion, but it can be difficult to relate the priors of these coefficients to the shape of $s$ [@lopez2018finite; @maatouk2017gaussian]. @lin2014bayesian introduce a method that projects unconstrained Gaussian processes onto a shape-constrained space. This approach has two limitations. It cannot conduct inference on covariance parameters as those posterior distributions are affected by the projection, and the projection often produces non-smooth sample paths (which reduces interpretability) [@golchi2015monotone]. Both limitations make it undesirable for this project. @lenk2017bayesian assume the q^th^ derivative of $s$ are squares of Gaussian processes, where $q = 1$ for monotonicity and $q = 2$ for convexity. They place priors on the coefficients of a Karhunen-Loeve expansion, which are not particularly interpretable. Many basis expansions have been proposed -- @zhou2020truncated list Bernstein polynomials, regression splines, penalized spines, cumulative distribution functions, and restricted splines -- but priors on these coefficients are also not particularly interpretable. Finally, @shively2009bayesian uses a mixture of constrained normals $N^{*}(0,c\sigma^{2}\Sigma)$ as the prior on the coefficients of a spline regression to encourage monotonicity. However, this prior can be difficult to interpret -- the constrained normal $N^{*}$ can be hard to explain as the dimension of $\Sigma$ increases, and the scale parameter $c$ has to be tuned by the user

Finally, consider an approach similar in spirit to our own. @burkner2020modelling propose a Bayesian model to estimate ordinal predictors with monotonic effects. They employ a simplex parameter $\zeta$ to model normalized differences between categories, and a scale parameter $b$. The prior on $b$ expresses prior knowledge on the average differences between adjacent categories, while the prior on $\zeta$ expresses prior knowledge on individual differences between adjacent categories. The authors suggest an $N(0,\sigma)$ prior on $b$ and a $\text{Dirichlet}(\alpha)$ prior on $\zeta$. Then, $\sigma$ and $\alpha$ would express how heavily average and individual differences between adjacent categories are penalized. Not only are $\zeta$ and $b$ interpretable, but so are the prior parameters $\sigma$ and $\alpha$. The bcGAIM seeks to achieve this ease of interpretation of its parameters and priors. This will encourage adoption of the bcGAIM in other research areas, which is one of the goals of this project.

One paper that considers constraints is @xia2006cumulative, where the authors constrain $s$ to be monotonic and $\alpha$ to be non-decreasing. Another is @fawzi2016structured, where the authors constrain $\alpha$ to be non-negative and sum to one but do not constrain $s$. In comparison, the cGAIM allows for any linear constraint on $\alpha$ and different shape constraints on $s$ including monotonicity, convexity, and concavity [@pierre2020aim]. Finally, while there are R packages, such as $\texttt{scam}$ and $\texttt{cgam}$ that facilitate shape-constrained inference, they do not estimate $\alpha$ [@pya2015shape; @liao2019cgam]. The cGAIM considers shape constrained inference of $s$ while estimating $\alpha$ under a variety of possible constraints. The bcGAIM will also allow users to specify a variety of constraints on $\alpha$ and $s$ simultaneously, and report posterior distributions that communicate estimation uncertainty for both.




### applications








**References**
<!-- Source: https://stackoverflow.com/questions/46738721/how-can-i-bring-back-indentation-workaround-in-rmarkdown-pandoc-genuine-referenc -->
\setlength{\parskip}{1pt}
<div id="refs"></div>
\setlength{\parskip}{8pt}
